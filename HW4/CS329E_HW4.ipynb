{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C S 329E HW 4\n",
    "\n",
    "## Pair Programming Group Number: 4\n",
    "## Members of Team: Ella Jiang, Alex Chiu\n",
    "\n",
    "## Decision Tree Classifier\n",
    "\n",
    "\n",
    "For this weeks homework we are going to explore ideas around decision tree implementation!  \n",
    "\n",
    "We will implement some helper functions that would be necessary for a home-grown tree:\n",
    "  - calc_entropy\n",
    "  - calc_gini\n",
    "  \n",
    "and them test them out at given data splits. \n",
    "  \n",
    "And finally, to perform predictive and descriptive analytics we use the [Decision Tree Classifier](https://scikit-learn.org/stable/modules/tree.html#classification) class in the scikit-learn package.\n",
    "\n",
    "  \n",
    "For this assignment, the stopping condition will be the depth of the tree. The impurity measure can be either `Entropy` or `Gini`.\n",
    "\n",
    "To test our tree built from the Decision Tree Classifier class, we will revisit our Melbourne housing data (that has been cleaned and pruned) and use the files:\n",
    "\n",
    "   - `melb_tree_train.csv` for training the decision tree (we'll also see what happens if we use the same data to test as we used to train the data in the last problem)\n",
    "   - `melb_tree_test.csv` for testing the decision tree\n",
    "\n",
    "There are 10 features in these dataframes that we can use to describe and predict the class label housing \"Type\", which is 'h' house, 'u' duplex, or 't' townhome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from math import log2\n",
    "from sklearn import tree # you'll probably need to install this - look at Q6 for a link \n",
    "import graphviz # you'll probably need to install this - look at Q6 for a link "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Load the Data\n",
    "Load in the melb_tree_train.csv into a dataframe, and split that dataframe into `df_X`, which contains the features of the data set (everything but `Type`), and `s_y`, the series containing just the class label (just `Type`). The lengths of `df_X` and `s_y` should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"melb_tree_train.csv\")\n",
    "df_X = df.drop(\"Type\", 1)\n",
    "#s_y = df[\"Type\"]\n",
    "s_y= pd.DataFrame(df[\"Type\"], columns = [\"Type\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Implement a function to calculate entropy \n",
    "Implement a function `calc_entropy` that takes the the class label series, `s_y`, as a parameter. Implement using the definition on p128 in the DM book and only use pandas and log2 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_entropy(s_y) definition\n",
    "\n",
    "def calc_entropy(s_y):\n",
    "    n = len(s_y)\n",
    "    entropy = 0;\n",
    "    for i in s_y.Type.unique():\n",
    "        ni = int(s_y[s_y[\"Type\"] == i].count())\n",
    "        if ni == 0:\n",
    "            continue\n",
    "        p = ni/n\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Use the entropy function to\n",
    "  - (a) Calculate the entropy of the entire training set\n",
    "  - (b) Calculate the entropy of the three partitions formed from \n",
    "    * Landsize $\\in$ [0,200]\n",
    "    * Landsize $\\in$ (200,450]\n",
    "    * Landsize $\\in$ (450, $\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire data set\n",
    "calc_entropy(s_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less than or equal to 200\n",
    "less200 = pd.DataFrame(df[df[\"Landsize\"] <= 200][\"Type\"])\n",
    "calc_entropy(less200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between 200 and 450\n",
    "btwn200450 = pd.DataFrame(df[(df[\"Landsize\"] > 200) & (df[\"Landsize\"] <= 450)][\"Type\"])\n",
    "calc_entropy(btwn200450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greater than 450\n",
    "grt450 = pd.DataFrame(df[df[\"Landsize\"] > 450][\"Type\"])\n",
    "calc_entropy(grt450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Implement a function to calculate the Gini Index\n",
    "Implement the function `calc_gini` that takes the class label series, `s_y`, as a parameter. Implement using the definition on p128 in the DM book and only use the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gini(s_y):\n",
    "    n = len(s_y)\n",
    "    gini = 1;\n",
    "    for i in s_y.Type.unique():\n",
    "        ni = int(s_y[s_y[\"Type\"] == i].count())\n",
    "        if ni == 0:\n",
    "            continue\n",
    "        p = ni/n\n",
    "        gini -= p**2\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Use the Gini Index function to\n",
    "  - (a) Calculate the Gini index of the entire training set\n",
    "  - (b) Calculate the Gini index of the three partitions formed from \n",
    "    * Landsize $\\in$ [0,200]\n",
    "    * Landsize $\\in$ (200,450]\n",
    "    * Landsize $\\in$ (450, $\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire data set\n",
    "calc_gini(s_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less than or equal to 200\n",
    "calc_gini(less200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between 200 and 450\n",
    "calc_gini(btwn200450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greater than 450\n",
    "calc_gini(grt450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 Create a decision tree \n",
    "Using [scikit-learn](https://scikit-learn.org/stable/modules/tree.html#tree) create a multi class classifer for the data set using the Entropy impurity measure and a max depth of 3.\n",
    "\n",
    "Note that scikit-learn's algorithm doesn't handle categorical data, so that needs to be preprocessed using an one hot encoding.\n",
    "\n",
    "Display the tree using `export_text` from sklearn.tree, and use that information to write some descriptive analytics on the classification of houses.  For extra fun, use the export_graphviz to draw the graph (see documentation on the [scikit-learn webpage](https://scikit-learn.org/stable/modules/tree.html#classification)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tree\n",
    "ohe = pd.get_dummies(df_X[\"CouncilArea\"])\n",
    "df_Xohe= pd.concat([df_X.drop(\"CouncilArea\", 1), ohe],axis=1)\n",
    "\n",
    "#s_yohe = pd.get_dummies(s_y)\n",
    "#s_y\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3, criterion=\"entropy\")\n",
    "clf = clf.fit(df_Xohe, s_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display text version of the tree\n",
    "\n",
    "#feature_names=df_Xohe.columns\n",
    "#plt.figure(figsize=(100,310),dpi=300)\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (16,5), dpi=300)\n",
    "tree.plot_tree(clf,fontsize=10,feature_names=df_Xohe.columns, filled=True, class_names = [\"h\",\"t\",\"u\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display graphviz version of the tree\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,feature_names=df_Xohe.columns, filled=True, class_names = [\"h\",\"t\",\"u\"])  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Answer containing your descriptive analytics in markdown here ⬅️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 Calculate the Accuracy and Display Learning Curve\n",
    "Load in the test data from melb_tree_test.csv.\n",
    "\n",
    "Use the scikit-learn library to create many decision trees, each one with a different configuration (aka Hyperparameters).  You will create 28 different trees by:\n",
    "\n",
    "    - Varying the max depth from 2 to 15 with the Gini Index as the impurity measure\n",
    "    - Varying the max depth from 2 to 15 with the Entropy as the impurity measure\n",
    "\n",
    "Implementation tip: you can create an array of numbers from 2 to 15 by using the numpy function [arange](https://numpy.org/doc/stable/reference/generated/numpy.arange.html).\n",
    "\n",
    "For each of the 28 decistion trees, calculate the error rate by using the data in the: \n",
    "  - Training set, and\n",
    "  - Test set.\n",
    "\n",
    "Display the results graphicaly, and offer an analysis of the trend (or if no trend present, offer a hypotheisis of why).  The max depth should be on the x-axis, and the error rate should be on the y-axis (see figure 3.23 in your DM textbook for a similar style of graph that uses leaf nodes instead of depth for the x-axis). Your plot will include 4 series of data\n",
    "   - Test error (entropy)\n",
    "   - Test error (gini index)\n",
    "   - Training error (entropy)\n",
    "   - Training error (gini index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the test data\n",
    "dfTest = pd.read_csv(\"melb_tree_test.csv\")\n",
    "y_test = pd.DataFrame(dfTest[\"Type\"])\n",
    "dfTest_X = dfTest.drop(\"Type\", 1)\n",
    "ohe = pd.get_dummies(dfTest_X[\"CouncilArea\"])\n",
    "dfTest_X= pd.concat([dfTest_X.drop(\"CouncilArea\",1),ohe],axis=1)\n",
    "\n",
    "#dfTest_X.columns==df_Xohe.columns\n",
    "\n",
    "y_test=y_test.to_numpy().transpose()\n",
    "y_test=y_test.reshape(y_test.shape[1],)\n",
    "\n",
    "s_y=s_y.to_numpy().transpose()\n",
    "s_y=s_y.reshape(s_y.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the trees using the training data\n",
    "giniTrees = []\n",
    "entropyTrees=[]\n",
    "giniTestPredict = []\n",
    "entropyTestPredict = []\n",
    "giniTrainingPredict = []\n",
    "entropyTrainingPredict = []\n",
    "z =16\n",
    "for i in range(2,z):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i, criterion=\"entropy\")\n",
    "    clf = clf.fit(df_Xohe, s_y)\n",
    "    entropyTrees.append(clf)\n",
    "    entropyTestPredict.append(clf.predict(dfTest_X))\n",
    "    entropyTrainingPredict.append(clf.predict(df_Xohe))\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i,criterion=\"gini\")\n",
    "    clf = clf.fit(df_Xohe, s_y)\n",
    "    giniTrees.append(clf)\n",
    "    giniTestPredict.append(clf.predict(dfTest_X))\n",
    "    giniTrainingPredict.append(clf.predict(df_Xohe))\n",
    "\n",
    "#fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (16,5), dpi=300)\n",
    "#tree.plot_tree(giniTrees[3],fontsize=10,feature_names=df_Xohe.columns, filled=True, class_names = [\"h\",\"t\",\"u\"])\n",
    "entropyTestError = []\n",
    "giniTestError = []\n",
    "entropyTrainingError = []\n",
    "giniTrainingError = []\n",
    "for i in range(len(entropyTrees)):\n",
    "    ne = len(y_test)\n",
    "    nf = len(s_y)\n",
    "    e = 0\n",
    "    f = 0\n",
    "    for j in range(ne):\n",
    "        if entropyTestPredict[i][j]!=y_test[j]:\n",
    "            e+=1\n",
    "    for j in range(nf):\n",
    "        if entropyTrainingPredict[i][j]!=s_y[j]:\n",
    "            f+=1\n",
    "    entropyTestError.append(e/ne)\n",
    "    entropyTrainingError.append(f/nf)\n",
    "    #print(str(f) + \" \" +str(f/nf))\n",
    "    e = 0\n",
    "    f = 0\n",
    "    for j in range(ne):\n",
    "        if giniTestPredict[i][j]!=y_test[j]:\n",
    "            e+=1\n",
    "    for j in range(nf):\n",
    "        if giniTrainingPredict[i][j]!=s_y[j]:\n",
    "            f+=1\n",
    "    giniTestError.append(e/ne)\n",
    "    giniTrainingError.append(f/nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 4 learning curves\n",
    "x = range(2,z)\n",
    "plt.plot(x,entropyTestError)\n",
    "plt.plot(x,giniTestError)\n",
    "plt.plot(x,entropyTrainingError)\n",
    "plt.plot(x,giniTrainingError)\n",
    "plt.xlabel(\"Levels\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend([\"Entropy Test Error\",\"Gini Test Error\",\"Entropy Training Error\",\"Gini Training Error\"])\n",
    "plt.title(\"Learning Curves\")\n",
    "#see page 196 of textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that as the depth of the decision trees increased, the training error for both the Entropy and Gini measurements shrank, while the test error either stayed the same or grew. This is a clear demonstration of the tradeoff between under- and overfitting, as even though all error shrinks initially, as the layers of the tree increase, we start overfitting the training data, making our model unable to accurately represent the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
